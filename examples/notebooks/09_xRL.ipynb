{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cebaac41",
   "metadata": {},
   "source": [
    "# **Explainable Reinforcement Learning Tutorial**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc29597",
   "metadata": {},
   "source": [
    "Welcome to this tutorial on **Explainable Reinforcement Learning (XRL)**! In this guide, we'll explore how to interpret and explain the decisions made by reinforcement learning agents using the SHAP (SHapley Additive exPlanations) library. We'll work through a practical example involving an the simulation simulation in a reinforcement learning setting, and demonstrate how to compute and visualize feature attributions for the agent's actions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75891d5",
   "metadata": {},
   "source": [
    "## **Table of Contents**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37317c5",
   "metadata": {},
   "source": [
    "1. [Introduction](#introduction)\n",
    "\n",
    "    1.1. [Multi-Agent Deep Reinforcement Learning](#11-multi-agent-deep-reinforcement-learning)\n",
    "\n",
    "    1.2. Prerequisites\n",
    "\n",
    "2. [Explainable AI and SHAP Values](#2-explainable-ai-and-shap-values)\n",
    "\n",
    "    2.1 Understanding Explainable AI \n",
    "\n",
    "    2.2 Introduction to SHAP Values \n",
    "\n",
    "3. [Calculating SHAP values](#3-calculating-shap-values)\n",
    "\n",
    "    3.1. [Loading and Preparing Data](#loading-and-preparing-data)\n",
    "\n",
    "    3.2. [Creating a SHAP Explainer](#32-creating-a-shap-explainer)\n",
    "    \n",
    "4. [Visualizing SHAP Values](#visualizing-shap-values)\n",
    "5. [Conclusion](#conclusion)\n",
    "6. [Additional Resources](#additional-resources)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0843ff0",
   "metadata": {},
   "source": [
    "## 1. Introduction <a name=\"introduction\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785bb1f0",
   "metadata": {},
   "source": [
    "Reinforcement Learning (RL) has achieved remarkable success in various domains, such as game playing, robotics, and autonomous systems. However, RL models, particularly those using deep neural networks, are often seen as black boxes due to their complex architectures and non-linear computations. This opacity poses challenges in understanding and trusting the decisions made by RL agents, especially in critical applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc036da",
   "metadata": {},
   "source": [
    "**Explainable Reinforcement Learning (XRL)** aims to bridge this gap by providing insights into the agent's decision-making process. By leveraging explainability techniques, we can interpret the actions of an RL agent, understand the influence of input features, and potentially improve the model's performance and fairness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe37845f",
   "metadata": {},
   "source": [
    "In this tutorial, we will demonstrate how to apply SHAP values to a trained actor neural network within an RL framework to explain the agent's actions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1925d195",
   "metadata": {},
   "source": [
    "### 1.1 Multi-Agent Deep Reinforcement Learning <a name=\"MARL\"></a>\n",
    "\n",
    "In ASSUME, we implement RL agents using a Multi-Agent Deep Reinforcement Learning (MADRL) approach. Key aspects include:\n",
    "\n",
    "\n",
    "- **Observations**: Each agent receives observations comprising market forecasts, unit-specific information, and past actions.\n",
    "- **Actions**: Agents decide on bidding strategies, such as bid prices for inflexible and flexible capacities.\n",
    "- **Rewards**: Agents receive rewards based on profits and opportunity costs, guiding them to learn optimal bidding strategies.\n",
    "- **Algorithm**: We utilize a multi-agent version of the TD3 algorithm, ensuring stable learning in a non-stationary environment.\n",
    "\n",
    "For a deep dive into the RL configurations we refer to one of the other tutorials, such as \n",
    "[Deep Reinforcement Learning Tutorial](https://example.com/deep-rl-tutorial)\n",
    "\n",
    "Agents need observations to make informed decisions. Observations include:\n",
    "\n",
    "- **Residual Load Forecast**: Forecasted net demand over the next 24 hours.\n",
    "- **Price Forecast**: Forecasted market prices over the next 24 hours.\n",
    "- **Marginal Cost**: Current marginal cost of the unit.\n",
    "- **Previous Output**: Dispatched capacity from the previous time step.\n",
    "\n",
    "\n",
    "Agents choose actions based on the observations. The action space is two-dimensional, corresponding to:\n",
    "\n",
    "- Bid Price for Inflexible Capacity (p_inflex): The price at which the agent offers its minimum power output (must-run capacity) to the market.\n",
    "- Bid Price for Flexible Capacity (p_flex): The price for the additional capacity above the minimum output that the agent can flexibly adjust.\n",
    "\n",
    "\n",
    "#### Run an the simulation MADRL Simulation\n",
    "\n",
    "Similar to the other tutorial, we can run Assume in the following way. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a12091",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install 'assume-framework[learning]'\n",
    "#!git clone https://github.com/assume-framework/assume.git assume-repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5901ff14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib.util\n",
    "\n",
    "# Check if 'google.colab' is available\n",
    "IN_COLAB = importlib.util.find_spec(\"google.colab\") is not None\n",
    "\n",
    "colab_inputs_path = \"assume-repo/examples/inputs\"\n",
    "local_inputs_path = \"../inputs\"\n",
    "\n",
    "inputs_path = colab_inputs_path if IN_COLAB else local_inputs_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2ab497",
   "metadata": {},
   "source": [
    "For XRL, we need enhanced logging of the learning process, which is not currently a feature of ASSUME itself. Therefore, we are overriding some functions to enable this logging specifically for the purpose of this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f126d51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Overwrite run_learning function with enhanced logging\n",
    "\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import yaml\n",
    "from tqdm import tqdm\n",
    "\n",
    "from assume.common.exceptions import AssumeException\n",
    "from assume.scenario.loader_csv import (\n",
    "    load_config_and_create_forecaster,\n",
    "    setup_world,\n",
    ")\n",
    "from assume.world import World\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def run_learning(\n",
    "    world: World,\n",
    "    inputs_path: str,\n",
    "    scenario: str,\n",
    "    study_case: str,\n",
    "    verbose: bool = False,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Train Deep Reinforcement Learning (DRL) agents to act in a simulated market environment.\n",
    "\n",
    "    This function runs multiple episodes of simulation to train DRL agents, performs evaluation, and saves the best runs. It maintains the buffer and learned agents in memory to avoid resetting them with each new run.\n",
    "\n",
    "    Args:\n",
    "        world (World): An instance of the World class representing the simulation environment.\n",
    "        inputs_path (str): The path to the folder containing input files necessary for the simulation.\n",
    "        scenario (str): The name of the scenario for the simulation.\n",
    "        study_case (str): The specific study case for the simulation.\n",
    "\n",
    "    Note:\n",
    "        - The function uses a ReplayBuffer to store experiences for training the DRL agents.\n",
    "        - It iterates through training episodes, updating the agents and evaluating their performance at regular intervals.\n",
    "        - Initial exploration is active at the beginning and is disabled after a certain number of episodes to improve the performance of DRL algorithms.\n",
    "        - Upon completion of training, the function performs an evaluation run using the best policy learned during training.\n",
    "        - The best policies are chosen based on the average reward obtained during the evaluation runs, and they are saved for future use.\n",
    "    \"\"\"\n",
    "    from assume.reinforcement_learning.buffer import ReplayBuffer\n",
    "\n",
    "    if not verbose:\n",
    "        logger.setLevel(logging.WARNING)\n",
    "\n",
    "    # remove csv path so that nothing is written while learning\n",
    "    temp_csv_path = world.export_csv_path\n",
    "    world.export_csv_path = \"\"\n",
    "\n",
    "    # initialize policies already here to set the obs_dim and act_dim in the learning role\n",
    "    actors_and_critics = None\n",
    "    world.learning_role.initialize_policy(actors_and_critics=actors_and_critics)\n",
    "    world.output_role.del_similar_runs()\n",
    "\n",
    "    # check if we already stored policies for this simualtion\n",
    "    save_path = world.learning_config[\"trained_policies_save_path\"]\n",
    "\n",
    "    if Path(save_path).is_dir():\n",
    "        # we are in learning mode and about to train new policies, which might overwrite existing ones\n",
    "        accept = input(\n",
    "            f\"{save_path=} exists - should we overwrite current learnings? (y/N) \"\n",
    "        )\n",
    "        if not accept.lower().startswith(\"y\"):\n",
    "            # stop here - do not start learning or save anything\n",
    "            raise AssumeException(\"don't overwrite existing strategies\")\n",
    "\n",
    "    # -----------------------------------------\n",
    "    # Load scenario data to reuse across episodes\n",
    "    scenario_data = load_config_and_create_forecaster(inputs_path, scenario, study_case)\n",
    "\n",
    "    # -----------------------------------------\n",
    "    # Information that needs to be stored across episodes, aka one simulation run\n",
    "    inter_episodic_data = {\n",
    "        \"buffer\": ReplayBuffer(\n",
    "            buffer_size=int(world.learning_config.get(\"replay_buffer_size\", 5e5)),\n",
    "            obs_dim=world.learning_role.rl_algorithm.obs_dim,\n",
    "            act_dim=world.learning_role.rl_algorithm.act_dim,\n",
    "            n_rl_units=len(world.learning_role.rl_strats),\n",
    "            device=world.learning_role.device,\n",
    "            float_type=world.learning_role.float_type,\n",
    "        ),\n",
    "        \"actors_and_critics\": None,\n",
    "        \"max_eval\": defaultdict(lambda: -1e9),\n",
    "        \"all_eval\": defaultdict(list),\n",
    "        \"avg_all_eval\": [],\n",
    "        \"episodes_done\": 0,\n",
    "        \"eval_episodes_done\": 0,\n",
    "        \"noise_scale\": world.learning_config.get(\"noise_scale\", 1.0),\n",
    "    }\n",
    "\n",
    "    # -----------------------------------------\n",
    "\n",
    "    validation_interval = min(\n",
    "        world.learning_role.training_episodes,\n",
    "        world.learning_config.get(\"validation_episodes_interval\", 5),\n",
    "    )\n",
    "\n",
    "    eval_episode = 1\n",
    "\n",
    "    for episode in tqdm(\n",
    "        range(1, world.learning_role.training_episodes + 1),\n",
    "        desc=\"Training Episodes\",\n",
    "    ):\n",
    "        # TODO normally, loading twice should not create issues, somehow a scheduling issue is raised currently\n",
    "        if episode != 1:\n",
    "            setup_world(\n",
    "                world=world,\n",
    "                scenario_data=scenario_data,\n",
    "                study_case=study_case,\n",
    "                episode=episode,\n",
    "            )\n",
    "\n",
    "        # -----------------------------------------\n",
    "        # Give the newly initliazed learning role the needed information across episodes\n",
    "        world.learning_role.load_inter_episodic_data(inter_episodic_data)\n",
    "\n",
    "        world.run()\n",
    "\n",
    "        # -----------------------------------------\n",
    "        # Store updated information across episodes\n",
    "        inter_episodic_data = world.learning_role.get_inter_episodic_data()\n",
    "        inter_episodic_data[\"episodes_done\"] = episode\n",
    "\n",
    "        # evaluation run:\n",
    "        if (\n",
    "            episode % validation_interval == 0\n",
    "            and episode\n",
    "            >= world.learning_role.episodes_collecting_initial_experience\n",
    "            + validation_interval\n",
    "        ):\n",
    "            world.reset()\n",
    "\n",
    "            # load evaluation run\n",
    "            setup_world(\n",
    "                world=world,\n",
    "                scenario_data=scenario_data,\n",
    "                study_case=study_case,\n",
    "                perform_evaluation=True,\n",
    "                eval_episode=eval_episode,\n",
    "            )\n",
    "\n",
    "            world.learning_role.load_inter_episodic_data(inter_episodic_data)\n",
    "\n",
    "            world.run()\n",
    "\n",
    "            total_rewards = world.output_role.get_sum_reward()\n",
    "            avg_reward = np.mean(total_rewards)\n",
    "            # check reward improvement in evaluation run\n",
    "            # and store best run in eval folder\n",
    "            terminate = world.learning_role.compare_and_save_policies(\n",
    "                {\"avg_reward\": avg_reward}\n",
    "            )\n",
    "\n",
    "            inter_episodic_data[\"eval_episodes_done\"] = eval_episode\n",
    "\n",
    "            # if we have not improved in the last x evaluations, we stop loop\n",
    "            if terminate:\n",
    "                break\n",
    "\n",
    "            eval_episode += 1\n",
    "\n",
    "        world.reset()\n",
    "\n",
    "        # if at end of simulation save last policies\n",
    "        if episode == (world.learning_role.training_episodes):\n",
    "            world.learning_role.rl_algorithm.save_params(\n",
    "                directory=f\"{world.learning_role.trained_policies_save_path}/last_policies\"\n",
    "            )\n",
    "\n",
    "            # export buffer_obs.json in the last training episode to get observations later\n",
    "            export = inter_episodic_data[\"buffer\"].observations.tolist()\n",
    "            path = f\"{world.learning_role.trained_policies_save_path}/buffer_obs\"\n",
    "            os.makedirs(path, exist_ok=True)\n",
    "            with open(os.path.join(path, \"buffer_obs.json\"), \"w\") as f:\n",
    "                json.dump(export, f)\n",
    "\n",
    "        # container shutdown implicitly with new initialisation\n",
    "    logger.info(\"################\")\n",
    "    logger.info(\"Training finished, Start evaluation run\")\n",
    "    world.export_csv_path = temp_csv_path\n",
    "\n",
    "    world.reset()\n",
    "\n",
    "    # load scenario for evaluation\n",
    "    setup_world(\n",
    "        world=world,\n",
    "        scenario_data=scenario_data,\n",
    "        study_case=study_case,\n",
    "        terminate_learning=True,\n",
    "    )\n",
    "\n",
    "    world.learning_role.load_inter_episodic_data(inter_episodic_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06983d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!cd assume-repo && assume -s example_02a -db \"sqlite:///./examples/local_db/assume_db_example_02a.db\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5801abff",
   "metadata": {},
   "source": [
    "### 1.2. Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98029fc",
   "metadata": {},
   "source": [
    "To follow along with this tutorial, we need some additional libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc5f403",
   "metadata": {},
   "source": [
    "- `matplotlib`\n",
    "- `shap`\n",
    "- `scikit-learn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f9335d",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!pip install matplotlib\n",
    "!pip install shap==0.42.1\n",
    "!pip install scikit-learn==1.3.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aedbacf",
   "metadata": {},
   "source": [
    "## 2. Explainable AI and SHAP Values <a name=\"explainable-ai-and-shap-values\"></a>\n",
    "\n",
    "### 2.1 Understanding Explainable AI \n",
    "Explainable AI (XAI) refers to techniques and methods that make the behavior and decisions of AI systems understandable to humans. In the context of complex models like deep neural networks, XAI helps to:\n",
    "- Increase Transparency: Providing insights into how models make decisions.\n",
    "- Build Trust: Users and stakeholders can trust AI systems if they understand them.\n",
    "- Ensure Compliance: Regulatory requirements often demand explainability.\n",
    "- Improve Models: Identifying weaknesses or biases in models.\n",
    "\n",
    "\n",
    "### 2.2 Introduction to SHAP Values \n",
    "Shapley values are a method from cooperative game theory used to explain the contribution of each feature to the prediction of a machine learning model, such as a neural network. They provide an interpretability technique by distributing the \"payout\" (the prediction) among the input features, attributing the importance of each feature to the prediction.\n",
    "\n",
    "For a given prediction, the Shapley value of a feature represents the average contribution of that feature to the prediction, considering all possible combinations of other features.\n",
    "\n",
    "1. **Marginal Contribution**: \n",
    "   The marginal contribution of a feature is the difference between the prediction with and without that feature.\n",
    "\n",
    "2. **Average over all subsets**: \n",
    "   The Shapley value is calculated by averaging the marginal contributions over all possible subsets of features.\n",
    "\n",
    "The formula for the Shapley value of feature $i$ is:\n",
    "\n",
    "$$\n",
    "\\phi_i = \\sum_{S \\subseteq N \\setminus \\{i\\}} \\frac{|S|!(|N| - |S| - 1)!}{|N|!} \\cdot \\left( f(S \\cup \\{i\\}) - f(S) \\right)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $N$ is the set of all features.\n",
    "- $S$ is a subset of features.\n",
    "- $f(S)$ is the model’s prediction when using only the features in subset $S$.\n",
    "\n",
    "\n",
    "The `shap` library is a popular tool for computing Shapley values for machine learning models, including neural networks.\n",
    "\n",
    "\n",
    "\n",
    "Why Use SHAP in RL?\n",
    "- Model-Agnostic: Applicable to any machine learning model, including neural networks.\n",
    "- Local Explanations: Provides explanations for individual predictions (actions).\n",
    "- Consistency: Ensures that features contributing more to the prediction have higher Shapley values.\n",
    "\n",
    "\n",
    "Properties of SHAP:\n",
    "1. Local Accuracy: The sum of Shapley values equals the difference between the model output and the expected output.\n",
    "2. Missingness: Features not present in the model have zero Shapley value.\n",
    "3. Consistency: If a model changes so that a feature contributes more to the prediction, the Shapley value of that feature should not decrease."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06e3f87",
   "metadata": {},
   "source": [
    "## 3. Calculating SHAP values <a name=\"calculating-shap-values\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a5f6f7",
   "metadata": {},
   "source": [
    "We will work with:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43627028",
   "metadata": {},
   "source": [
    "- **Observations (`input_data`)**: These are the inputs to our actor neural network, representing the state of the environment.\n",
    "- **Trained Actor Model**: A neural network representing the decision making of one RL power plant that outputs actions based on the observations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518ef8b6",
   "metadata": {},
   "source": [
    "Our goal is to:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce6c5da",
   "metadata": {},
   "source": [
    "- Load the observations and the trained actor model.\n",
    "- Use the model to predict actions.\n",
    "- Apply SHAP to explain the model's predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d171982",
   "metadata": {},
   "source": [
    "### 3.1. Loading and Preparing Data <a name=\"loading-and-preparing-data\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b3efac",
   "metadata": {},
   "source": [
    "First, let's load the necessary libraries and the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29daf714",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import shap\n",
    "import torch as th\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c45521",
   "metadata": {},
   "source": [
    "the simulation common.py contains utility functions and class definitions\n",
    "from common import load_observations, Actor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc34fb2",
   "metadata": {},
   "source": [
    "**Define the Actor Neural Network Class**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc629a7f",
   "metadata": {},
   "source": [
    "We define the actor neural network class that will be used to predict actions based on observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af54695",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "from assume.reinforcement_learning.neural_network_architecture import MLPActor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2645dac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_config(file_path):\n",
    "    \"\"\"\n",
    "    Load the configuration file.\n",
    "    \"\"\"\n",
    "    with open(file_path) as file:\n",
    "        config = yaml.safe_load(file)\n",
    "    return config\n",
    "\n",
    "\n",
    "# Some Variable definitions:\n",
    "\n",
    "EPISODES = 3\n",
    "NUMBER_OF_AGENTS = 1\n",
    "SIM_TIMESPAN_DAYS = 31\n",
    "ACTOR_NUM = 1\n",
    "EXAMPLE = 1\n",
    "\n",
    "SIM_TIMESPAN_HOURS = SIM_TIMESPAN_DAYS * 24\n",
    "\n",
    "# actor 1-5 are the default non-rl actors, so we just skip those\n",
    "ACTOR_NUM_ADJ = ACTOR_NUM + 6  # 6 #9\n",
    "\n",
    "\n",
    "# Get the current working directory\n",
    "current_dir = os.getcwd()\n",
    "# Go up one level\n",
    "one_level_up = os.path.dirname(current_dir)\n",
    "# Go up two levels\n",
    "two_levels_up = os.path.dirname(one_level_up)\n",
    "\n",
    "# Paths\n",
    "path = os.path.join(\n",
    "    two_levels_up,\n",
    "    f\"assume/examples/output/{EXAMPLE}/{EPISODES}_episodes_{SIM_TIMESPAN_DAYS}_simDays_{NUMBER_OF_AGENTS}_rlAgents\",\n",
    ")\n",
    "actor_path = os.path.join(path, f\"actor_pp_{ACTOR_NUM_ADJ}.pt\")\n",
    "\n",
    "# DEFINTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e3bd23",
   "metadata": {},
   "source": [
    "We define a utility function to load observations and input data from a specified path. Analyzing the shap values for all observations and all parameters would make this notebook quite lengthy, so we’re filtering the observation data frame to include only 700 observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc97137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Load observations function\n",
    "\n",
    "\n",
    "def load_observations(path, ACTOR_NUM, feature_names):\n",
    "    # Load observations\n",
    "    obs_path = f\"{path}/buffer_obs.json\"\n",
    "\n",
    "    with open(obs_path) as file:\n",
    "        json_data = json.load(file)\n",
    "\n",
    "    # Convert the list of lists into a 2D numpy array\n",
    "    input_data = np.array(json_data)\n",
    "    input_data = np.squeeze(input_data)\n",
    "\n",
    "    # filter out arrays where all value are 0\n",
    "    input_data = input_data[~np.all(input_data == 0, axis=1)]\n",
    "\n",
    "    # filter only first 700 observations\n",
    "    input_data = input_data[:700]\n",
    "\n",
    "    if NUMBER_OF_AGENTS == 1:\n",
    "        return pd.DataFrame(input_data, columns=feature_names), input_data\n",
    "    else:\n",
    "        return pd.DataFrame(\n",
    "            input_data[:, ACTOR_NUM], columns=feature_names\n",
    "        ), input_data[:, ACTOR_NUM]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5cfb11",
   "metadata": {},
   "source": [
    "**Define Paths and Parameters**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f2588b",
   "metadata": {},
   "source": [
    "Adjust the following paths and parameters according to your data and model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb61f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = (\n",
    "    inputs_path + \"/example_02a/learned_strategies/base/buffer_obs\"\n",
    ")  # Replace with your data path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74fda15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature names (replace with actual feature names)\n",
    "# make columns names\n",
    "names_1 = [\"price forecast t+\" + str(x) for x in range(1, 25)]\n",
    "names_2 = [\"residual load forecast t+\" + str(x) for x in range(1, 25)]\n",
    "feature_names = names_1 + names_2 + [\"total capacity t-1\"] + [\"marginal costs t-1\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ad06bf",
   "metadata": {},
   "source": [
    "**Load Observations and Input Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b62582",
   "metadata": {},
   "source": [
    "Load the observations and input data using the utility function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a88f7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_obs, input_data = load_observations(path, ACTOR_NUM, feature_names)\n",
    "\n",
    "df_obs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e23a9db",
   "metadata": {},
   "source": [
    "**Load the Trained Actor Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307f6c75",
   "metadata": {},
   "source": [
    "We initialize and load the trained actor neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af59fd50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "obs_dim = len(feature_names)\n",
    "act_dim = 2  # Adjust if your model outputs a different number of actions\n",
    "model = MLPActor(obs_dim=obs_dim, act_dim=act_dim, float_type=th.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a266a519",
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTOR_NUM = 1  # Replace with the actor number or identifier\n",
    "actor_path = (\n",
    "    inputs_path\n",
    "    + \"/example_02a/learned_strategies/base/last_policies/actors/actor_pp_6.pt\"\n",
    ")  # Path to the trained actor model\n",
    "\n",
    "# Load the trained model parameters\n",
    "model_state = th.load(actor_path, map_location=th.device(\"cpu\"))\n",
    "model.load_state_dict(model_state[\"actor\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ddc767",
   "metadata": {},
   "source": [
    "Get the actions base on observation tensor we just loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8811c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "for obs in input_data:\n",
    "    obs_tensor = th.tensor(obs, dtype=th.float)\n",
    "    prediction = model(obs_tensor)\n",
    "    predictions.append(prediction)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c98944",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    input_data, predictions, test_size=0.15, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b87c86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to tensors\n",
    "y_train = th.stack(y_train)\n",
    "y_test = th.stack(y_test)\n",
    "\n",
    "X_train_tensor = th.tensor(X_train, dtype=th.float32)\n",
    "y_train_tensor = th.tensor(y_train, dtype=th.float32)\n",
    "X_test_tensor = th.tensor(X_test, dtype=th.float32)\n",
    "y_test_tensor = th.tensor(y_test, dtype=th.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8492ae",
   "metadata": {},
   "source": [
    "## 3.2. Creating a SHAP Explainer <a name=\"creating-a-shap-explainer\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4593e44e",
   "metadata": {},
   "source": [
    "We define a prediction function compatible with SHAP and create a Kernel SHAP explainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580e1672",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Define a prediction function for SHAP\n",
    "def model_predict(X):\n",
    "    X_tensor = th.tensor(X, dtype=th.float32)\n",
    "    model.eval()\n",
    "    with th.no_grad():\n",
    "        return model(X_tensor).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05477da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a subset of training data for the background dataset\n",
    "background_size = 100  # Adjust the size as needed\n",
    "background = X_train[:background_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0008ecd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the SHAP Kernel Explainer\n",
    "explainer = shap.KernelExplainer(model_predict, background)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf909424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate SHAP values for the test set\n",
    "shap_values = explainer.shap_values(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9acbad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc5d38b",
   "metadata": {},
   "source": [
    "## 4. Visualizing SHAP Values <a name=\"visualizing-shap-values\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428b64dc",
   "metadata": {},
   "source": [
    "We generate summary plots to visualize feature importance for each output dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59362ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(shap_values[0].shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce62fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary plot for the first output dimension\n",
    "shap.summary_plot(shap_values[0], X_test, feature_names=feature_names, show=False)\n",
    "plt.title(\"Summary Plot for Output Dimension 0, p_inflex\")\n",
    "plt.show()\n",
    "\n",
    "# Summary plot for the second output dimension\n",
    "shap.summary_plot(shap_values[1], X_test, feature_names=feature_names, show=False)\n",
    "plt.title(\"Summary Plot for Output Dimension 1, p_flex\")\n",
    "plt.show()\n",
    "\n",
    "shap.summary_plot(\n",
    "    shap_values[0],\n",
    "    X_test,\n",
    "    feature_names=feature_names,\n",
    "    plot_type=\"bar\",\n",
    "    title=\"Summary Bar Plot for Output Dimension 0\",\n",
    ")\n",
    "\n",
    "shap.summary_plot(\n",
    "    shap_values[1],\n",
    "    X_test,\n",
    "    feature_names=feature_names,\n",
    "    plot_type=\"bar\",\n",
    "    title=\"Summary Bar Plot for Output Dimension 1\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba561615",
   "metadata": {},
   "source": [
    "The SHAP summary plots show the impact of each feature on the model's predictions for each output dimension (action). Features with larger absolute SHAP values have a more significant influence on the decision-making process of the RL agent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889daaf2",
   "metadata": {},
   "source": [
    "- **Positive SHAP Value**: Indicates that the feature contributes positively to the predicted action value.\n",
    "- **Negative SHAP Value**: Indicates that the feature contributes negatively to the predicted action value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df44601",
   "metadata": {},
   "source": [
    "By analyzing these plots, we can identify which features are most influential and understand how changes in feature values affect the agent's actions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495b2ee0",
   "metadata": {},
   "source": [
    "## 5. Conclusion <a name=\"conclusion\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e486c6b",
   "metadata": {},
   "source": [
    "In this tutorial, we've demonstrated how to apply SHAP to a reinforcement learning agent to explain its decision-making process. By interpreting the SHAP values, we gain valuable insights into which features influence the agent's actions, enhancing transparency and trust in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed715458",
   "metadata": {},
   "source": [
    "Explainability is crucial, especially when deploying RL agents in real-world applications where understanding the rationale behind decisions is essential for safety, fairness, and compliance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57873800",
   "metadata": {},
   "source": [
    "## 6. Additional Resources <a name=\"additional-resources\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515e10f7",
   "metadata": {},
   "source": [
    "- **SHAP Documentation**: [https://shap.readthedocs.io/en/latest/](https://shap.readthedocs.io/en/latest/)\n",
    "- **PyTorch Documentation**: [https://pytorch.org/docs/stable/index.html](https://pytorch.org/docs/stable/index.html)\n",
    "- **Reinforcement Learning Introduction**: [Richard S. Sutton and Andrew G. Barto, \"Reinforcement Learning: An Introduction\"](http://incompleteideas.net/book/the-book-2nd.html)\n",
    "- **Interpretable Machine Learning Book**: [https://christophm.github.io/interpretable-ml-book/](https://christophm.github.io/interpretable-ml-book/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17a7c09",
   "metadata": {},
   "source": [
    "**Feel free to experiment with the code and explore different explainability techniques. Happy learning!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
